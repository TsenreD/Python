{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87bbcb45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:06.602513Z",
     "start_time": "2024-04-07T14:56:05.674769Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:38:07.658778Z",
     "iopub.status.busy": "2024-04-09T16:38:07.658427Z",
     "iopub.status.idle": "2024-04-09T16:38:58.266364Z",
     "shell.execute_reply": "2024-04-09T16:38:58.265009Z"
    },
    "id": "o4vBVdNx2EPr",
    "papermill": {
     "duration": 50.621262,
     "end_time": "2024-04-09T16:38:58.269070",
     "exception": false,
     "start_time": "2024-04-09T16:38:07.647808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.15).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\r\n",
      "--2024-04-09 16:38:13--  https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week06_policy_based/atari_wrappers.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 13919 (14K) [text/plain]\r\n",
      "Saving to: 'atari_wrappers.py'\r\n",
      "\r\n",
      "atari_wrappers.py   100%[===================>]  13.59K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2024-04-09 16:38:13 (63.3 MB/s) - 'atari_wrappers.py' saved [13919/13919]\r\n",
      "\r\n",
      "--2024-04-09 16:38:14--  https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week06_policy_based/env_batch.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 8090 (7.9K) [text/plain]\r\n",
      "Saving to: 'env_batch.py'\r\n",
      "\r\n",
      "env_batch.py        100%[===================>]   7.90K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2024-04-09 16:38:14 (67.0 MB/s) - 'env_batch.py' saved [8090/8090]\r\n",
      "\r\n",
      "--2024-04-09 16:38:15--  https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week06_policy_based/runners.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2534 (2.5K) [text/plain]\r\n",
      "Saving to: 'runners.py'\r\n",
      "\r\n",
      "runners.py          100%[===================>]   2.47K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2024-04-09 16:38:15 (42.6 MB/s) - 'runners.py' saved [2534/2534]\r\n",
      "\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "kaggle-environments 1.14.3 requires shimmy>=1.2.1, but you have shimmy 0.2.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "init = True\n",
    "if init:\n",
    "    # Install xvfb and our launcher script for it\n",
    "    !apt-get install -y xvfb\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/xvfb -O ../xvfb\n",
    "\n",
    "    # Download dependencies from Github\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week06_policy_based/atari_wrappers.py\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week06_policy_based/env_batch.py\n",
    "    !wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week06_policy_based/runners.py\n",
    "\n",
    "    # Update the gym environment to be compatible with the Atari environment\n",
    "    !pip install -q gymnasium[atari,accept-rom-license]\n",
    "    !pip install -q tensorboardX\n",
    "\n",
    "    !touch .setup_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8928be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T16:38:58.291044Z",
     "iopub.status.busy": "2024-04-09T16:38:58.290662Z",
     "iopub.status.idle": "2024-04-09T16:38:58.305056Z",
     "shell.execute_reply": "2024-04-09T16:38:58.304178Z"
    },
    "papermill": {
     "duration": 0.027749,
     "end_time": "2024-04-09T16:38:58.307074",
     "exception": false,
     "start_time": "2024-04-09T16:38:58.279325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bc25e",
   "metadata": {
    "id": "O_iJbFWQ2EPs",
    "papermill": {
     "duration": 0.009909,
     "end_time": "2024-04-09T16:38:58.327115",
     "exception": false,
     "start_time": "2024-04-09T16:38:58.317206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementing Advantage-Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba2d03",
   "metadata": {
    "id": "16ownLDJ2EPs",
    "papermill": {
     "duration": 0.009792,
     "end_time": "2024-04-09T16:38:58.346840",
     "exception": false,
     "start_time": "2024-04-09T16:38:58.337048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel.\n",
    "\n",
    "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscale, take max between frames, skip frames and stack them together) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
    "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function. Note that if you are using\n",
    "PyTorch and not using `tensorboardX` you will need to implement a wrapper that will log **raw** total rewards that the *unwrapped* environment returns and redefine the implemention of `nature_dqn_env` function here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6481374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:14.473897Z",
     "start_time": "2024-04-07T14:56:06.604391Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:38:58.369424Z",
     "iopub.status.busy": "2024-04-09T16:38:58.368707Z",
     "iopub.status.idle": "2024-04-09T16:39:11.192005Z",
     "shell.execute_reply": "2024-04-09T16:39:11.190553Z"
    },
    "id": "uScP-zu12EPt",
    "papermill": {
     "duration": 12.837913,
     "end_time": "2024-04-09T16:39:11.194524",
     "exception": false,
     "start_time": "2024-04-09T16:38:58.356611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from atari_wrappers import nature_dqn_env\n",
    "\n",
    "\n",
    "env_name = \"SpaceInvadersNoFrameskip-v4\"\n",
    "nenvs = multiprocessing.cpu_count() \n",
    "summaries = \"Numpy\"\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "env = nature_dqn_env(env_name, nenvs=nenvs, summaries=summaries)\n",
    "obs, _ = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab95218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.314696Z",
     "start_time": "2024-04-07T14:56:14.474951Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:11.218363Z",
     "iopub.status.busy": "2024-04-09T16:39:11.217007Z",
     "iopub.status.idle": "2024-04-09T16:39:11.524325Z",
     "shell.execute_reply": "2024-04-09T16:39:11.523448Z"
    },
    "id": "C9rPGodqqUOa",
    "outputId": "4d2d64df-5df9-43ce-8ead-15400512f5de",
    "papermill": {
     "duration": 0.321473,
     "end_time": "2024-04-09T16:39:11.526513",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.205040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x78381920f9d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuGElEQVR4nO3de3BUZZ7/8U8CSRMM6UCUDgECkUWjXBYJEiK4uhIX0UHUlLVaKkHwhkFuNaAZF9RymKDOiuMNdFTAC6LMiIKusE5UECeABFERDTiwEi4JzgzpRiQBk/P7w7J/nD4NSSfdebrj+1X1VM33Oc85+fKkJ19PP+cSZ1mWJQAAWlm86QQAAL9MFCAAgBEUIACAERQgAIARFCAAgBEUIACAERQgAIARFCAAgBEUIACAERQgAIAREStATz31lHr37q0OHTooNzdXmzZtitSPAgDEoLhIPAvutdde07hx47Rw4ULl5ubqscce0/Lly1VRUaGuXbuect+Ghgbt379fnTp1UlxcXLhTAwBEmGVZOnz4sDIyMhQff4rzHCsChg4dahUVFfnj+vp6KyMjwyopKWl038rKSksSjUaj0WK8VVZWnvLvfdi/gjt27JjKy8uVn5/v74uPj1d+fr7Kysoc4+vq6uTz+fzN4uHcANAmdOrU6ZTbw16A/v73v6u+vl4ej8fW7/F4VFVV5RhfUlIit9vtb5mZmeFOCQBgQGPLKMavgisuLpbX6/W3yspK0ykBAFpB+3Af8PTTT1e7du1UXV1t66+urlZ6erpjvMvlksvlCncaAIAoF/YzoMTEROXk5Ki0tNTf19DQoNLSUuXl5YX7xwEAYlTYz4AkacaMGSosLNSQIUM0dOhQPfbYYzpy5IhuvvnmSPw4AEAMikgB+s///E999913mjNnjqqqqjRo0CCtXr3acWECAOCXKyI3oraEz+eT2+02nQYAoIW8Xq9SUlJOut34VXAAgF8mChAAwAgKEADACAoQAMAIChAAwAgKEADACAoQAMAIChAAwIiIPAmhNQwaNKhVfs7u3bsdfV6v1xaPGjXqlHEkzZgxo9Exjz76aCtk0rRczj33XFucmJgYkVxqamocff/3f/8XkZ8VKRdddFGr/Jxt27Y5+v7xj3/Y4vHjx58yjqSLL7640TEffvhhxPOQmpZLbm6uLe7QoUNEcgn8HUnBf5fRjDMgAIARFCAAgBEUIACAETG7BtQUW7duPeX2wPUIKXJrEl988YUtXrRokWNM586dbfHs2bMjkkugaFpHkpzrboFrbpIcT1bv1q1bRHOKVmvXrj3l9sD1CClyaxIfffSRLQ72+Q38vb322msRySVQNK0jSc61mmDrOb1797bFvXr1imRKRnAGBAAwggIEADCCAgQAMCJm14DGjRvX6JjG1oAKCgocfYHrMC+88IJjTOCaRODPqa6uduwT7L6UQN9//70tfvHFFxvdpynCcZxw5RI454HzLTnnPNgaUOB9YIH3XgWuuUnB192i2QMPPNDomMbWNoKt7wWuw9x7772OMR9//LEtDlwfCXZ/XLB1jECB/z+47777Gt2nKcJxnHDlEjjnwd4EHTjngfMtOX+3gfdeBa65SdwHBABAk1CAAABGUIAAAEZQgAAARsTsRQhN0Zo3TzYmNTXVFrfWw1SbIppykaQJEyaYTiFmtObNk41JS0uzxa31MNWmiKZcJGnu3LmmU4gKnAEBAIygAAEAjKAAAQCMiLMsyzKdxIl8Pp/cbrfpNAAALeT1epWSknLS7ZwBAQCMoAABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMCLkArVu3TmPGjFFGRobi4uL05ptv2rZblqU5c+aoW7duSkpKUn5+vnbu3BmufAEAbUTIBejIkSP613/9Vz311FNBtz/88MN6/PHHtXDhQm3cuFGnnXaaRo0apdra2hYnCwBoQ6wWkGStWLHCHzc0NFjp6enWI4884u+rqamxXC6X9eqrrzbpmF6v15JEo9FotBhvXq/3lH/vw7oGtHv3blVVVSk/P9/f53a7lZubq7KysqD71NXVyefz2RoAoO0LawGqqqqSJHk8Hlu/x+PxbwtUUlIit9vtbz179gxnSgCAKGX8Krji4mJ5vV5/q6ysNJ0SAKAVhLUApaenS5Kqq6tt/dXV1f5tgVwul1JSUmwNAND2hbUAZWVlKT09XaWlpf4+n8+njRs3Ki8vL5w/CgAQ49qHusP333+vb775xh/v3r1bW7duVZcuXZSZmalp06bpt7/9rfr27ausrCzNnj1bGRkZuuqqq8KZNwAg1oV66fUHH3wQ9HK7wsJC/6XYs2fPtjwej+VyuayRI0daFRUVTT4+l2HTaDRa22iNXYbNG1Gj2PLly23xtddeaygTaeDAgY6+zz//3EAmwd1+++22+JlnnjGUiTR9+nRH3/z58w1kEtxZZ51li3fs2GEok+j6jCP8eCMqACAqUYAAAEZQgAAARoR8FRwaN27cOEff9u3bbbHL5XKMGT9+vC2O1PfhkyZNssVdunRxjAm8l+u5556LSC6rV6929K1du9YWr1u3zjEm8IkZ4VrzSUhIsMXJycm2ePjw4Y59Tnz0lCRNmzYtLLkE+uijjxx9e/futcVPPvmkLQ62vhOONZ9o/4wjNnAGBAAwggIEADCCAgQAMIICBAAwghtRW0ngQvoLL7zgGPPPf/7TFt900022OFwLtoELwcEW1lNTU23xoEGDHGP69u0blnwCVVRU2OJNmzY5xhw6dMgWf/nll7Y4XBclvPTSS7b4nXfecYwJ/LzeeOONjjEXXnhhi3Pp0aOHoy/wgo2kpCRbHDhPknT11Vfb4nDdiBpNn3FEB25EBQBEJQoQAMAIChAAwAhuRG0lga8pD/weXnLeBHn55ZdHJJfevXvb4q5duza6T2s+eDTwJthg3yGfe+65tnjKlCkRyWXo0KGN5hLo0UcfjUguHTt2dPRt27btlPsMHjzY0Reph49G02ccsYEzIACAERQgAIARFCAAgBHcBxQBgd+FS9K//Mu/2OKPP/7YMSbwfpz6+npbvGHDhjBk5/w5+/btc4w5cuSILQ58iZkU/N8QqmAvuuvUqVOjPyfw33DgwAFbvGvXrmblExcXZ4svuOACW7xlyxbHPn369LHFwT6/4ZirYPdr1dXV2eLNmzc3uk/get7hw4dDziXaP+OIDtwHBACIShQgAIARFCAAgBEUIACAEVyEAACICC5CAABEJQoQAMAIChAAwAgKEADACAoQAMAIChAAwAgKEADACAoQAMAI3ojaSjp37myLA9/oKUm1tbW2uLy8PKI5/SzYE5MDheNpzk2VnZ1ti9PS0hxjvv32W1u8d+/eiOb0s2ibq8byCXw6tuR8gna4RPNnHNGJMyAAgBEUIACAESEVoJKSEp1//vnq1KmTunbtqquuukoVFRW2MbW1tSoqKlJaWpqSk5NVUFCg6urqsCYNAIh9IT2M9LLLLtN1112n888/Xz/++KN+85vfaNu2bdq+fbtOO+00SdKkSZP0zjvvaPHixXK73Zo8ebLi4+Ob/L14W30Y6apVq2xxbm6uY8wtt9xii1euXBmRXO6//35b3LFjR8eYrKwsWxwf7/xvlYKCgrDm9bPp06fb4uLiYseYZcuW2eIpU6ZEJJfA/8D6/vvvHWOeffZZW3zw4EHHmBUrVrQ4l2BvpQ38Pb300ku2+LvvvnPs069fvxbnEkw0fcYRHRp7GGlIFyGsXr3aFi9evFhdu3ZVeXm5/u3f/k1er1fPP/+8li5dqksuuUSStGjRIp1zzjnasGGDhg0b1ox/AgCgLWrRGpDX65UkdenSRdJPV7QcP35c+fn5/jHZ2dnKzMxUWVlZ0GPU1dXJ5/PZGgCg7Wt2AWpoaNC0adM0fPhw9e/fX5JUVVWlxMREpaam2sZ6PB5VVVUFPU5JSYncbre/9ezZs7kpAQBiSLNfSDdp0iS9++67Wr9+vXr06CFJWrp0qW6++WbHfQZDhw7Vv//7v+uhhx5yHKeurs423ufzxXwRGjdunKNv+/btttjlcjnGjB8/3hZffvnltrh79+4tT04//e5O9PMZ7IkCLxz5+uuvHWPWr1/f4lwCv9aVpLVr19ridevWOcYEfkYGDx5si2fNmtWsfBISEmxxcnKyLQ52382JZ/ySlJiY6Bhz5513NiufE3300UeOvsD7n5588klbvGPHDsc+gXO3ZcuWkHOJ9s84okNY14B+NnnyZL399ttat26dv/hIUnp6uo4dO6aamhrbWVB1dbXS09ODHsvlcgX9oAIA2raQvoKzLEuTJ0/WihUr9P777zuuwMnJyVFCQoJKS0v9fRUVFdqzZ4/y8vLCkzEAoE0I6QyoqKhIS5cu1VtvvaVOnTr513XcbreSkpLkdrs1ceJEzZgxQ126dFFKSoruuusu5eXlcQUcAMAmpAK0YMECSdLFF19s61+0aJH/u9358+crPj5eBQUFqqur06hRo/T000+HJVkAQNsRUgFqyvUKHTp00FNPPaWnnnqq2UkBANo+ngUHADCCAgQAMIICBAAwghfSRUBTnv4d7OGsgTfp/fWvfw1XSjb79u2zxUePHnWM2b17ty2O1COSPv/880bHBJur6667zhb/7W9/C0s+DQ0NtvjHH3+0xVu3bnXsM3ToUFt86NChsOQS6NNPP3X0Bb4ELnCuzjjjDMc+wR6oGqpo/4wjNnAGBAAwggIEADCCAgQAMIICBAAwotlPw46UtvpGVAD4pWnsadicAQEAjKAAAQCMoAABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMoAABAIzgjagRMG7cOEffqlWrbHGwt2bm5OTY4pUrV9ri7t27hyE7adKkSbb43XffdYw5cuSILT777LMdY9avX9/iXFavXu3omzZtmi3++uuvHWN69Ohhi6dMmWKLZ82a1ax8EhISbHFycrItrq2tdezTp08fW3znnXc6xgTrC9VHH33k6Js+fbot3rx5sy12uVyOffr162eLt2zZEnIu0f4ZR2zgDAgAYAQFCABgBAUIAGAEa0ARsGbNGkdfcXGxLR4/frxjTGFhoS0uKCgIa14/69Kliy0Otj6RlZVli/ft2+cYE441oGBrNUOHDrXF69atc4x55JFHbPHChQtbnIsk/fjjj7b46aeftsVnnXWWY59nn33WFjdnTaUpHnroIUdfWlqaLT548KAtDra+N3ny5BbnEu2fccQGzoAAAEZQgAAARlCAAABGUIAAAEbEWZZlmU7iRD6fT26323QaERe4WCxJXbt2NZCJ88ZUSRowYIAtDseNlM0VTXMV7GbQwMX3cFyc0VSdO3e2xRUVFbbY1DxJ0fV7gxler1cpKSkn3c4ZEADACAoQAMCIkArQggULNHDgQKWkpCglJUV5eXm2+wxqa2tVVFSktLQ0JScnq6CgQNXV1WFPGgAQ+0K6EbVHjx6aN2+e+vbtK8uytGTJEo0dO1affvqp+vXrp+nTp+udd97R8uXL5Xa7NXnyZF1zzTX6+OOPI5V/VBo1apSjb+TIkbY42Hfho0ePtsUTJkywxddee20YspPeeustWzx16lTHmD/96U+n3EeSxo4d2+JcHn74YUfftm3bbHGwuZo5c6Yt9vl8tviZZ55pVj7t2rWzxW+88YYt/o//+A/HPoEPI43UXAU77oMPPmiLA+dqyZIljn3mzp1ri3fs2BFyLtH+GUdsCKkAjRkzxhbPnTtXCxYs0IYNG9SjRw89//zzWrp0qS655BJJ0qJFi3TOOedow4YNGjZsWPiyBgDEvGavAdXX12vZsmU6cuSI8vLyVF5eruPHjys/P98/Jjs7W5mZmSorKzvpcerq6uTz+WwNAND2hVyAvvjiCyUnJ8vlcumOO+7QihUrdO6556qqqkqJiYlKTU21jfd4PKqqqjrp8UpKSuR2u/2tZ8+eIf8jAACxJ+QCdPbZZ2vr1q3auHGjJk2apMLCQm3fvr3ZCRQXF8vr9fpbZWVls48FAIghVguNHDnSuu2226zS0lJLknXo0CHb9szMTOvRRx9t8vG8Xq8liUaj0Wgx3rxe7yn/3rf4PqCGhgbV1dUpJydHCQkJKi0t9W+rqKjQnj17lJeX19IfAwBoY0K6Cq64uFijR49WZmamDh8+rKVLl+rDDz/UmjVr5Ha7NXHiRM2YMUNdunRRSkqK7rrrLuXl5XEFHADAIaQCdPDgQY0bN04HDhyQ2+3WwIEDtWbNGl166aWSpPnz5ys+Pl4FBQWqq6vTqFGjHC/0AgBA4mGkAIAI4WGkAICoRAECABhBAQIAGEEBAgAYQQECABhBAQIAGEEBAgAYQQECABgR0pMQ0Hzr1q2zxdnZ2Y4xdXV1tjhSr6YYP368LQ72VtJACxcudPTNmTMnXCnZVFRU2OLOnTs7xgQ+NT0nJyciubz00ku2ONibQAMFe6vn2rVrW5xLjx49HH1btmw55T6bN2929F1++eUtziWYaPqMIzZwBgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwghtRW0lJSYktfvfddx1jysvLWyWXxYsX2+LevXs7xnz22We2eMWKFRHMyG7Dhg22uLCw0DFm5syZrZLLTTfdZIvPOussx5jA3204bjoNZu/evY6+Q4cO2eKzzz7bFmdkZEQkl2Ci6TOO2MAZEADACAoQAMAIChAAwAjWgCLA4/E4+gYNGmSLg30/ftlll9niYcOG2eLAtZHmuvfee23x/fff3+g+jz32mKNv2rRpLc5l4MCBjr4nnnii0f0CH0Z65pln2uJdu3Y1K5+4uDhbvHTpUlt8/fXXO/YpKCiwxbfccotjzHPPPdesfE70q1/9ytEXuOYT6IEHHnD0zZgxwxYfPnw45Fyi/TOO2MAZEADACAoQAMAIChAAwIg4y7Is00mcyOfzye12m04DANBCXq9XKSkpJ93OGRAAwAgKEADACAoQAMAIChAAwAgKEADACAoQAMAIChAAwIgWFaB58+YpLi7O9kyw2tpaFRUVKS0tTcnJySooKFB1dXVL8wQAtDHNLkCffPKJnnnmGcfDJKdPn65Vq1Zp+fLlWrt2rfbv369rrrmmxYkCANoYqxkOHz5s9e3b13rvvfesiy66yJo6daplWZZVU1NjJSQkWMuXL/eP/eqrryxJVllZWZOO7fV6LUk0Go1Gi/Hm9XpP+fe+WWdARUVFuuKKK5Sfn2/rLy8v1/Hjx2392dnZyszMVFlZWdBj1dXVyefz2RoAoO0L+X1Ay5Yt05YtW/TJJ584tlVVVSkxMVGpqam2fo/Ho6qqqqDHKykpCfrOEgBA2xbSGVBlZaWmTp2qV155RR06dAhLAsXFxfJ6vf4W+KIxAEDbFNIZUHl5uQ4ePKjBgwf7++rr67Vu3To9+eSTWrNmjY4dO6aamhrbWVB1dbXS09ODHtPlcsnlcjUv+xgyffp0W1xcXOwY891339nifv36RSSX3r172+JNmzY1us8ll1zi6Nu2bVu4UrJZtWqVLc7NzXWMWbZsmS2eMmVKRHIZMmSILf6f//mfRvcJ9nsL/N02R1JSkqPv22+/PeU+wd7OunLlyhbnEkw0fcYRG0IqQCNHjtQXX3xh67v55puVnZ2tu+++Wz179lRCQoJKS0v9rymuqKjQnj17lJeXF76sAQAxL6QC1KlTJ/Xv39/Wd9pppyktLc3fP3HiRM2YMUNdunRRSkqK7rrrLuXl5Tne/Q4A+GUL+SKExsyfP1/x8fEqKChQXV2dRo0apaeffjrcPwYAEON4I2oEjBs3ztH3+9//3hZv2bLFMWbChAm2OPBKw+7du4chO+ngwYONjunatWuj+wSOaY7Vq1c7+k5cY5SkX//6144xHo/HFp9xxhm2eNasWc3KJyEhwRbv27ev0X1aa66a8nsL/Ixs2LDBMebWW2+1xcE+i42J9s84ogNvRAUARCUKEADACAoQAMCIsF+EAOnFF1909I0cOdIWBz4tQnLe/xKp78MDLwoJ9l39Rx99ZIvDsYYRzGWXXeboGz16tC2+4447HGPWr19vi5u75hPo+PHjtvjQoUO2eM2aNY59du7caYsjNVcjRoxw9B04cMAW/+lPf7LFY8aMceyzf//+FucS7Z9xxAbOgAAARlCAAABGUIAAAEZQgAAARnARgiFjx4519JWXlxvIJPjDKdu1a2cgk+CCzdXMmTMNZBL8oad//vOfDWTyk8OHD9viwLnKyMhozXRsoukzjujEGRAAwAgKEADACAoQAMAIHkYKAIgIHkYKAIhKFCAAgBEUIACAERQgAIARFCAAgBEUIACAERQgAIARFCAAgBEUIACAERQgAIARFCAAgBEUIACAEbyQzpC33nrL0RfsBV6t4corr3T09enTxxbPnz+/tdJxiKa5evzxxx19zz33nC3+/PPPWysdderUyRa//PLLttjUPEnR9XtDdOIMCABgBAUIAGAEBQgAYAQFCABgBBchRMC4ceMcfddee60tzs3NdYwJvBhgwYIFtrh79+5hyE46ePCgLV68eLFjTFZWli0uLi52jOnatWuLc1m9erWjb82aNbY4Ly/PMSbwYoDa2lpbPGvWrGblk5CQYIv37dtniysrKx37fPnll7b4L3/5i2NMOOYq8PcmSTfddJMtDpyrwNyC7bNly5aQc4n2zzhiA2dAAAAjKEAAACNCKkD333+/4uLibC07O9u/vba2VkVFRUpLS1NycrIKCgpUXV0d9qQBALEv5DWgfv362b7jbt/+/x9i+vTpeuedd7R8+XK53W5NnjxZ11xzjT7++OPwZBsjXnzxRUffyJEjbXGwNYGMjAxbHKnvw59++mlbfP/99ze6z5///OeI5HLZZZc5+kaPHm2Lg83VzJkzbfEjjzwSlnyOHz9uiw8dOmSLc3JyGj1GsLWacBgxYoSjb8eOHbY4cK7Ky8sd+zRnzSdQtH/GERtCLkDt27dXenq6o9/r9er555/X0qVLdckll0iSFi1apHPOOUcbNmzQsGHDWp4tAKDNCHkNaOfOncrIyNCZZ56pG264QXv27JH0039pHT9+XPn5+f6x2dnZyszMVFlZ2UmPV1dXJ5/PZ2sAgLYvpAKUm5urxYsXa/Xq1VqwYIF2796tCy+8UIcPH1ZVVZUSExOVmppq28fj8aiqquqkxywpKZHb7fa3nj17NusfAgCILSF9BXfid/MDBw5Ubm6uevXqpddff11JSUnNSqC4uFgzZszwxz6fjyIEAL8ALboRNTU1VWeddZa++eYbXXrppTp27JhqampsZ0HV1dVB14x+5nK55HK5WpJG1Al2k15hYaEtHjx4sGNMOBaHm6IpVyYGXqhQUFAQkVyC3YgaeGHCww8/7BjT3BtNGxN4I2pT1i4DLw5YsWJFWHP62fPPP+/ou/DCC21x4I2zkVrkj/bPOGJDi+4D+v777/W3v/1N3bp1U05OjhISElRaWurfXlFRoT179gS9kx0A8MsW0hnQr3/9a40ZM0a9evXS/v37dd9996ldu3a6/vrr5Xa7NXHiRM2YMUNdunRRSkqK7rrrLuXl5XEFHADAIaQCtHfvXl1//fX6xz/+oTPOOEMjRozQhg0bdMYZZ0j66aVl8fHxKigoUF1dnUaNGuX4KgcAAEmKsyzLMp3EiXw+n9xut+k0WsTj8Tj6AtddBg4c6BjTWm/SDLwZMJj9+/e3QiZNm4emzGe4xMXF2eL4ePu31AMGDHDss3Xr1ojkEmjQoEGOvi+++MIWNzQ02OJI/d872j/jiA5er1cpKSkn3c6z4AAARlCAAABGUIAAAEawBgQAiAjWgAAAUYkCBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwggIEADCCAgQAMIICBAAwIuQCtG/fPt14441KS0tTUlKSBgwYoM2bN/u3W5alOXPmqFu3bkpKSlJ+fr527twZ1qQBALEvpAJ06NAhDR8+XAkJCXr33Xe1fft2/fd//7c6d+7sH/Pwww/r8ccf18KFC7Vx40addtppGjVqlGpra8OePAAghlkhuPvuu60RI0acdHtDQ4OVnp5uPfLII/6+mpoay+VyWa+++mqTfobX67Uk0Wg0Gi3Gm9frPeXf+5DOgFauXKkhQ4bo2muvVdeuXXXeeefpj3/8o3/77t27VVVVpfz8fH+f2+1Wbm6uysrKgh6zrq5OPp/P1gAAbV9IBWjXrl1asGCB+vbtqzVr1mjSpEmaMmWKlixZIkmqqqqSJHk8Htt+Ho/Hvy1QSUmJ3G63v/Xs2bM5/w4AQIwJqQA1NDRo8ODB+t3vfqfzzjtPt912m2699VYtXLiw2QkUFxfL6/X6W2VlZbOPBQCIHSEVoG7duuncc8+19Z1zzjnas2ePJCk9PV2SVF1dbRtTXV3t3xbI5XIpJSXF1gAAbV9IBWj48OGqqKiw9e3YsUO9evWSJGVlZSk9PV2lpaX+7T6fTxs3blReXl4Y0gUAtBlNu/7tJ5s2bbLat29vzZ0719q5c6f1yiuvWB07drRefvll/5h58+ZZqamp1ltvvWV9/vnn1tixY62srCzr6NGjXAVHo9Fov6DW2FVwIRUgy7KsVatWWf3797dcLpeVnZ1tPfvss7btDQ0N1uzZsy2Px2O5XC5r5MiRVkVFRZOPTwGi0Wi0ttEaK0BxlmVZiiI+n09ut9t0GgCAFvJ6vadc1+dZcAAAIyhAAAAjKEAAACPam04A0SktLc0W/+///m+j+0ycONHRt3Xr1nClFLXOOussW/zqq682us/YsWNt8d69e8OaU7S69NJLbfG8efMa3eeCCy6wxXV1dWHNCeZwBgQAMIICBAAwggIEADCCAgQAMIKLEBDU0KFDQ97n0UcfdfRdcskl4Ugnqs2YMSPkfQoKCmzxH/7wh3ClE9WactFBoO7du9viXbt2hSsdGMYZEADACAoQAMAIChAAwAgKEADACAoQAMAIChAAwAgKEADACAoQAMAIChAAwAgKEADACAoQAMAIChAAwIg4y7Is00mcyOfzye12m07jF6+8vDwsxwl8qGl9fX1YjhtNwjFXOTk5YcgkuvTv39/Rt2TJkhYfty3OVVvl9XqVkpJy0u2cAQEAjKAAAQCMoAABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMaG86AcSGptz8F66bV2Mdc3Vy+/fvt8VjxoxpdJ9f6lz9EnAGBAAwggIEADAipALUu3dvxcXFOVpRUZEkqba2VkVFRUpLS1NycrIKCgpUXV0dkcQBALEtpIeRfvfdd7aHSW7btk2XXnqpPvjgA1188cWaNGmS3nnnHS1evFhut1uTJ09WfHy8Pv744yYnxMNIAaBtaOxhpLJaYOrUqVafPn2shoYGq6amxkpISLCWL1/u3/7VV19ZkqyysrImH9Pr9VqSaDQajRbjzev1nvLvfbPXgI4dO6aXX35ZEyZMUFxcnMrLy3X8+HHl5+f7x2RnZyszM1NlZWUnPU5dXZ18Pp+tAQDavmYXoDfffFM1NTUaP368JKmqqkqJiYlKTU21jfN4PKqqqjrpcUpKSuR2u/2tZ8+ezU0JABBDml2Ann/+eY0ePVoZGRktSqC4uFher9ffKisrW3Q8AEBsaNaNqN9++63+8pe/6I033vD3paen69ixY6qpqbGdBVVXVys9Pf2kx3K5XHK5XM1JAwAQw5p1BrRo0SJ17dpVV1xxhb8vJydHCQkJKi0t9fdVVFRoz549ysvLa3mmAIA2JeQzoIaGBi1atEiFhYVq3/7/7+52uzVx4kTNmDFDXbp0UUpKiu666y7l5eVp2LBhYU0aANAGhHrp9Zo1ayxJVkVFhWPb0aNHrTvvvNPq3Lmz1bFjR+vqq6+2Dhw4ENLxuQybRqPR2kZr7DLskG5EbQ3ciAoAbUNjN6LyLDgAgBEUIACAERQgAIARFCAAgBEUIACAERQgAIARFCAAgBEUIACAEc16GCliy/Lly23xk08+aYvXrl3bmumgiVrrHvGxY8c6+lauXNkqPxu/bJwBAQCMoAABAIygAAEAjGANqI25/fbbHX2dO3e2xbfccostZg0oOl188cUtPsZrr73m6PN4PC0+LhAOnAEBAIygAAEAjKAAAQCMoAABAIzgIoQ25uyzzw55n1/96leOvrfffjsc6QDASXEGBAAwggIEADCCAgQAMII1oDZm4MCBjY7p1q2bLe7evXuk0kELfPjhh6ZTACKKMyAAgBEUIACAERQgAIARFCAAgBFxVmu9drGJfD6f3G53WI41ffp0W9ycmzSjXeCTjWtqahxj6urqWikbIHZceeWVtpi3wIbP0aNHNX36dHm9XqWkpJx0HGdAAAAjKEAAACMoQAAAI6L2RtT58+crKSnJdBpRz+Vy2eL4eP6bAmiKwBuy0fr4awUAMIICBAAwIqQCVF9fr9mzZysrK0tJSUnq06ePHnzwQZ14JbdlWZozZ466deumpKQk5efna+fOnWFPHAAQ20JaA3rooYe0YMECLVmyRP369dPmzZt18803y+12a8qUKZKkhx9+WI8//riWLFmirKwszZ49W6NGjdL27dvVoUOHiPwjfsmqq6tt8bFjxwxlAsQWXrpoXkgF6K9//avGjh2rK664QpLUu3dvvfrqq9q0aZOkn85+HnvsMf3Xf/2Xxo4dK0l68cUX5fF49Oabb+q6664Lc/oAgFgV0ldwF1xwgUpLS7Vjxw5J0meffab169dr9OjRkqTdu3erqqpK+fn5/n3cbrdyc3NVVlYW9Jh1dXXy+Xy2BgBo+0I6A7rnnnvk8/mUnZ2tdu3aqb6+XnPnztUNN9wgSaqqqpLkfDyMx+PxbwtUUlKiBx54oDm5AwBiWEhnQK+//rpeeeUVLV26VFu2bNGSJUv0+9//XkuWLGl2AsXFxfJ6vf5WWVnZ7GMBAGJHSGdAM2fO1D333ONfyxkwYIC+/fZblZSUqLCwUOnp6ZJ+Whg/8Sav6upqDRo0KOgxXS6X42ZKNB0PGgWaZ9++faZT+MUL6Qzohx9+cNxp365dOzU0NEiSsrKylJ6ertLSUv92n8+njRs3Ki8vLwzpAgDaipDOgMaMGaO5c+cqMzNT/fr106effqpHH31UEyZMkCTFxcVp2rRp+u1vf6u+ffv6L8POyMjQVVddFYn8AQAxKqQC9MQTT2j27Nm68847dfDgQWVkZOj222/XnDlz/GNmzZqlI0eO6LbbblNNTY1GjBih1atXcw8QAMAmal9Ix8NIASA28UI6AEBUowABAIygAAEAjKAAAQCMoAABAIygAAEAjKAAAQCMCOlG1Nbw821JtbW1hjMBADTHz3+/G7vNNOpuRN27d6969uxpOg0AQAtVVlaqR48eJ90edQWooaFB+/fvV6dOnXT48GH17NlTlZWVp7ybFs3j8/mY3whifiOL+Y2slsyvZVk6fPiwMjIyHA+wPlHUfQUXHx/vr5hxcXGSpJSUFD5gEcT8RhbzG1nMb2Q1d37dbnejY7gIAQBgBAUIAGBEVBcgl8ul++67jzemRgjzG1nMb2Qxv5HVGvMbdRchAAB+GaL6DAgA0HZRgAAARlCAAABGUIAAAEZQgAAARkRtAXrqqafUu3dvdejQQbm5udq0aZPplGJSSUmJzj//fHXq1Eldu3bVVVddpYqKCtuY2tpaFRUVKS0tTcnJySooKFB1dbWhjGPXvHnzFBcXp2nTpvn7mNuW27dvn2688UalpaUpKSlJAwYM0ObNm/3bLcvSnDlz1K1bNyUlJSk/P187d+40mHHsqK+v1+zZs5WVlaWkpCT16dNHDz74oO0hohGdXysKLVu2zEpMTLReeOEF68svv7RuvfVWKzU11aqurjadWswZNWqUtWjRImvbtm3W1q1brcsvv9zKzMy0vv/+e/+YO+64w+rZs6dVWlpqbd682Ro2bJh1wQUXGMw69mzatMnq3bu3NXDgQGvq1Kn+fua2Zf75z39avXr1ssaPH29t3LjR2rVrl7VmzRrrm2++8Y+ZN2+e5Xa7rTfffNP67LPPrCuvvNLKysqyjh49ajDz2DB37lwrLS3Nevvtt63du3dby5cvt5KTk60//OEP/jGRnN+oLEBDhw61ioqK/HF9fb2VkZFhlZSUGMyqbTh48KAlyVq7dq1lWZZVU1NjJSQkWMuXL/eP+eqrryxJVllZmak0Y8rhw4etvn37Wu+995510UUX+QsQc9tyd999tzVixIiTbm9oaLDS09OtRx55xN9XU1NjuVwu69VXX22NFGPaFVdcYU2YMMHWd80111g33HCDZVmRn9+o+wru2LFjKi8vV35+vr8vPj5e+fn5KisrM5hZ2+D1eiVJXbp0kSSVl5fr+PHjtvnOzs5WZmYm891ERUVFuuKKK2xzKDG34bBy5UoNGTJE1157rbp27arzzjtPf/zjH/3bd+/eraqqKtscu91u5ebmMsdNcMEFF6i0tFQ7duyQJH322Wdav369Ro8eLSny8xt1T8P++9//rvr6enk8Hlu/x+PR119/bSirtqGhoUHTpk3T8OHD1b9/f0lSVVWVEhMTlZqaahvr8XhUVVVlIMvYsmzZMm3ZskWffPKJYxtz23K7du3SggULNGPGDP3mN7/RJ598oilTpigxMVGFhYX+eQz294I5btw999wjn8+n7OxstWvXTvX19Zo7d65uuOEGSYr4/EZdAULkFBUVadu2bVq/fr3pVNqEyspKTZ06Ve+99546dOhgOp02qaGhQUOGDNHvfvc7SdJ5552nbdu2aeHChSosLDScXex7/fXX9corr2jp0qXq16+ftm7dqmnTpikjI6NV5jfqvoI7/fTT1a5dO8eVQtXV1UpPTzeUVeybPHmy3n77bX3wwQe2NxSmp6fr2LFjqqmpsY1nvhtXXl6ugwcPavDgwWrfvr3at2+vtWvX6vHHH1f79u3l8XiY2xbq1q2bzj33XFvfOeecoz179kiSfx75e9E8M2fO1D333KPrrrtOAwYM0E033aTp06erpKREUuTnN+oKUGJionJyclRaWurva2hoUGlpqfLy8gxmFpssy9LkyZO1YsUKvf/++8rKyrJtz8nJUUJCgm2+KyoqtGfPHua7ESNHjtQXX3yhrVu3+tuQIUN0ww03+P83c9syw4cPd9w2sGPHDvXq1UuSlJWVpfT0dNsc+3w+bdy4kTlugh9++MHxxtJ27dqpoaFBUivMb4svY4iAZcuWWS6Xy1q8eLG1fft267bbbrNSU1Otqqoq06nFnEmTJllut9v68MMPrQMHDvjbDz/84B9zxx13WJmZmdb7779vbd682crLy7Py8vIMZh27TrwKzrKY25batGmT1b59e2vu3LnWzp07rVdeecXq2LGj9fLLL/vHzJs3z0pNTbXeeust6/PPP7fGjh3LZdhNVFhYaHXv3t1/GfYbb7xhnX766dasWbP8YyI5v1FZgCzLsp544gkrMzPTSkxMtIYOHWpt2LDBdEoxSVLQtmjRIv+Yo0ePWnfeeafVuXNnq2PHjtbVV19tHThwwFzSMSywADG3Lbdq1Sqrf//+lsvlsrKzs61nn33Wtr2hocGaPXu25fF4LJfLZY0cOdKqqKgwlG1s8fl81tSpU63MzEyrQ4cO1plnnmnde++9Vl1dnX9MJOeX9wEBAIyIujUgAMAvAwUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGAEBQgAYAQFCABgBAUIAGDE/wOI/ixHph5AzwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = obs[0, 0, :]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9c1f7",
   "metadata": {
    "id": "jiWeYgmd2EPt",
    "papermill": {
     "duration": 0.010119,
     "end_time": "2024-04-09T16:39:11.547477",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.537358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we will need to implement a model that predicts logits and values. It is suggested that you use the same model as in [Nature DQN paper](https://www.nature.com/articles/nature14236) with a modification that instead of having a single output layer, it will have two output layers taking as input the output of the last hidden layer. **Note** that this model is different from the model you used in homework where you implemented DQN. You can use your favorite deep learning framework here. We suggest that you use orthogonal initialization with parameter $\\sqrt{2}$ for kernels and initialize biases with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d94db01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.344708Z",
     "start_time": "2024-04-07T14:56:15.319011Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:11.569974Z",
     "iopub.status.busy": "2024-04-09T16:39:11.569655Z",
     "iopub.status.idle": "2024-04-09T16:39:11.847609Z",
     "shell.execute_reply": "2024-04-09T16:39:11.846717Z"
    },
    "id": "FIkJ7z7TiWS4",
    "papermill": {
     "duration": 0.292636,
     "end_time": "2024-04-09T16:39:11.850682",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.558046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "N_FRAMES_STACKED = 4\n",
    "N_ACTIONS = env.action_space.n\n",
    "STATE_SHAPE = env.observation_space.shape\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ConvBackbone(nn.Sequential):\n",
    "    \"\"\"\n",
    "    The convolutional part of a DQN model.\n",
    "    Please, don't think about input scaling here: it will be implemented below.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in: int = N_FRAMES_STACKED) -> None:\n",
    "        super().__init__(\n",
    "            nn.Conv2d(c_in,out_channels=32, kernel_size=8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.to(device))\n",
    "\n",
    "class NatureBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the A2C model\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions, inp_size=64 * 7 * 7, hidden_size=512) -> None:\n",
    "        super().__init__()\n",
    "        self.action_size = n_actions\n",
    "\n",
    "        self.values = torch.nn.Sequential(\n",
    "            torch.nn.Linear(inp_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.logits = torch.nn.Sequential(\n",
    "            torch.nn.Linear(inp_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        for head in [self.values, self.logits]:\n",
    "            for m in head.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        assert x.ndim == 2, x.shape  # (batch_size, n_features)\n",
    "\n",
    "        return self.logits(x), self.values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75557c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.357507Z",
     "start_time": "2024-04-07T14:56:15.350028Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:11.879624Z",
     "iopub.status.busy": "2024-04-09T16:39:11.879191Z",
     "iopub.status.idle": "2024-04-09T16:39:11.884887Z",
     "shell.execute_reply": "2024-04-09T16:39:11.883300Z"
    },
    "id": "AmZlj7VdqUOd",
    "papermill": {
     "duration": 0.023548,
     "end_time": "2024-04-09T16:39:11.886975",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.863427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NatureDQN(nn.Sequential):\n",
    "    def __init__(self, c_in: int, n_actions: int) -> None:\n",
    "        backbone = ConvBackbone(c_in=c_in)\n",
    "        head = NatureBackbone(n_actions=n_actions)\n",
    "        super().__init__(backbone, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de45432",
   "metadata": {
    "id": "pA2VlyZ32EPt",
    "papermill": {
     "duration": 0.010275,
     "end_time": "2024-04-09T16:39:11.908301",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.898026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a dictionary of all the arrays that are needed to interact with an environment and train the model.\n",
    " Note that actions must be an `np.ndarray` while the other\n",
    "tensors need to have the type determined by your deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90765b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.368504Z",
     "start_time": "2024-04-07T14:56:15.360398Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:11.932968Z",
     "iopub.status.busy": "2024-04-09T16:39:11.932107Z",
     "iopub.status.idle": "2024-04-09T16:39:11.939037Z",
     "shell.execute_reply": "2024-04-09T16:39:11.938063Z"
    },
    "id": "dtHP-Fo72EPt",
    "papermill": {
     "duration": 0.022145,
     "end_time": "2024-04-09T16:39:11.941082",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.918937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        # Implement a policy by calling the model, sampling actions and computing their log probs.\n",
    "        # Should return a dict containing keys ['actions', 'logits', 'log_probs', 'values'].\n",
    "\n",
    "        logits, values = self.model(torch.from_numpy(inputs))\n",
    "        \n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        \n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        actions = dist.sample()\n",
    "        \n",
    "    \n",
    "        log_probs = dist.log_prob(actions)\n",
    "        \n",
    "        return {'actions': actions.detach().cpu(), 'log_probs': log_probs, 'entropy': entropy,\n",
    "                 'values': values, 'logits': logits}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834e18a",
   "metadata": {
    "id": "2oPCQwsd2EPt",
    "papermill": {
     "duration": 0.01316,
     "end_time": "2024-04-09T16:39:11.965973",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.952813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Next will pass the environment and policy to a runner that collects partial trajectories from the environment.\n",
    "The class that does is is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45834f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.374898Z",
     "start_time": "2024-04-07T14:56:15.369960Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:11.995708Z",
     "iopub.status.busy": "2024-04-09T16:39:11.994924Z",
     "iopub.status.idle": "2024-04-09T16:39:12.001410Z",
     "shell.execute_reply": "2024-04-09T16:39:12.000537Z"
    },
    "id": "fj-fKr_A2EPt",
    "papermill": {
     "duration": 0.024988,
     "end_time": "2024-04-09T16:39:12.003406",
     "exception": false,
     "start_time": "2024-04-09T16:39:11.978418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276661a",
   "metadata": {
    "id": "_9JehIbH2EPt",
    "papermill": {
     "duration": 0.010983,
     "end_time": "2024-04-09T16:39:12.030913",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.019930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys\n",
    "\n",
    "* 'observations'\n",
    "* 'rewards'\n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment. This list has length $T$ that is size of partial trajectory. Partial trajectory for given moment `t` is part of `ComputeValueTargets.__call__` input argument `trajectory` from moment `t` to the end (i.e. it's different at each iteration in the algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bfa76",
   "metadata": {
    "id": "iY7FB6s72EPu",
    "papermill": {
     "duration": 0.010474,
     "end_time": "2024-04-09T16:39:12.052187",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.041713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To train the part of the model that predicts state values you will need to compute the value targets.\n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected.\n",
    "Thus, we can implement and use `ComputeValueTargets` callable.\n",
    "The formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\left( \\sum_{t'=t}^{T - 1} \\gamma^{t'}r_{t'} \\right) + \\gamma^T \\hat{v}(s_{T}),\n",
    "$$\n",
    "\n",
    "In implementation, however, do not forget to use\n",
    "`trajectory['resets']` flags to check if you need to add the value targets at the next step when\n",
    "computing value targets for the current step. You can access `trajectory['state']['latest_observation']`\n",
    "to get last observations in partial trajectory &mdash; $s_{t+T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3bace5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.385177Z",
     "start_time": "2024-04-07T14:56:15.377030Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:12.075830Z",
     "iopub.status.busy": "2024-04-09T16:39:12.074986Z",
     "iopub.status.idle": "2024-04-09T16:39:12.084118Z",
     "shell.execute_reply": "2024-04-09T16:39:12.083169Z"
    },
    "id": "4CbDi3GZ2EPu",
    "papermill": {
     "duration": 0.023291,
     "end_time": "2024-04-09T16:39:12.086091",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.062800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, policy, gamma=0.99):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _calc_values(self, last_value, resets, rewards):\n",
    "        vs = []\n",
    "        T = len(rewards)\n",
    "        r = last_value\n",
    "        for step in reversed(range(T)):\n",
    "            r = rewards[step] + self.gamma * r * (~resets[step])\n",
    "            vs.insert(0, r)\n",
    "        \n",
    "        return vs\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        \"\"\"Compute value targets for a given partial trajectory.\"\"\"\n",
    "\n",
    "        # This method should modify trajectory inplace by adding\n",
    "        # an item with key 'value_targets' to it.\n",
    "\n",
    "        rewards = trajectory['rewards']\n",
    "        resets = trajectory['resets']\n",
    "        last_obs = trajectory['state']['latest_observation']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_value = self.policy.model(torch.from_numpy(last_obs))[1].squeeze()\n",
    "        \n",
    "\n",
    "        value_targets = self._calc_values(last_value.float(), torch.tensor(resets,  device=device), torch.tensor(rewards, dtype=torch.float32, device=device))\n",
    "        \n",
    "        \n",
    "        trajectory['value_targets'] = value_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5ff03",
   "metadata": {
    "id": "9_d9OYyz2EPu",
    "papermill": {
     "duration": 0.010532,
     "end_time": "2024-04-09T16:39:12.107793",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.097261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After computing value targets we will transform lists of interactions into tensors\n",
    "with the first dimension `batch_size` which is equal to `env_steps * num_envs`, i.e. you essentially need\n",
    "to flatten the first two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773016a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.400083Z",
     "start_time": "2024-04-07T14:56:15.392195Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:12.132153Z",
     "iopub.status.busy": "2024-04-09T16:39:12.131298Z",
     "iopub.status.idle": "2024-04-09T16:39:12.138787Z",
     "shell.execute_reply": "2024-04-09T16:39:12.137899Z"
    },
    "id": "IEnqWlHh2EPu",
    "papermill": {
     "duration": 0.022334,
     "end_time": "2024-04-09T16:39:12.140732",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.118398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Modify trajectory inplace.\n",
    "\n",
    "        flattened_interactions = {\n",
    "        key: torch.cat([self._get_new_value(v) for v in values]).squeeze()\n",
    "        for key, values in trajectory.items() if key != 'state'\n",
    "        }\n",
    "        trajectory.update(flattened_interactions)\n",
    "\n",
    "    def _get_new_value(self, value):\n",
    "        if isinstance(value, np.ndarray):\n",
    "            return torch.tensor(value, dtype=torch.float32, requires_grad=True, device=device)\n",
    "        elif len(value) == 0:\n",
    "            return torch.tensor(value)\n",
    "        return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726bf39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:12.162732Z",
     "iopub.status.busy": "2024-04-09T16:39:12.162470Z",
     "iopub.status.idle": "2024-04-09T16:39:12.166313Z",
     "shell.execute_reply": "2024-04-09T16:39:12.165467Z"
    },
    "papermill": {
     "duration": 0.016941,
     "end_time": "2024-04-09T16:39:12.168145",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.151204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_dict = {\n",
    "    \"nsteps\": 5,\n",
    "    \"optim\": \"RMSProp\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53055ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:12.190147Z",
     "iopub.status.busy": "2024-04-09T16:39:12.189898Z",
     "iopub.status.idle": "2024-04-09T16:39:48.534311Z",
     "shell.execute_reply": "2024-04-09T16:39:48.533313Z"
    },
    "papermill": {
     "duration": 36.358129,
     "end_time": "2024-04-09T16:39:48.536724",
     "exception": false,
     "start_time": "2024-04-09T16:39:12.178595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhallloy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240409_163917-zk0z1472\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisty-voice-38\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hallloy/a2c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hallloy/a2c/runs/zk0z1472\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret(\"wandb_key\")\n",
    "\n",
    "if init:\n",
    "    !wandb login --relogin {key}\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"a2c\",\n",
    "\n",
    "        # track hyperparameters and run metadata\n",
    "        config=parameter_dict\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f678ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.591612Z",
     "start_time": "2024-04-07T14:56:15.401173Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:48.562163Z",
     "iopub.status.busy": "2024-04-09T16:39:48.561286Z",
     "iopub.status.idle": "2024-04-09T16:39:49.587863Z",
     "shell.execute_reply": "2024-04-09T16:39:49.586791Z"
    },
    "id": "-2CwwzLl2EPu",
    "papermill": {
     "duration": 1.04438,
     "end_time": "2024-04-09T16:39:49.592811",
     "exception": false,
     "start_time": "2024-04-09T16:39:48.548431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NatureDQN(N_FRAMES_STACKED, N_ACTIONS).to(device)\n",
    "MODEL_PATH = '/kaggle/input/a2c/pytorch/clean/1/last_state_dict_400.pt'\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    nsteps=5,\n",
    "    transforms=[\n",
    "        ComputeValueTargets(policy),\n",
    "        MergeTimeBatch(),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01307f",
   "metadata": {
    "id": "IuYy-8Ri2EPu",
    "papermill": {
     "duration": 0.017502,
     "end_time": "2024-04-09T16:39:49.627409",
     "exception": false,
     "start_time": "2024-04-09T16:39:49.609907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into your lecture,\n",
    "[Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and [lecture](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) by Sergey Levine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f121ceb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:15.601407Z",
     "start_time": "2024-04-07T14:56:15.592660Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:49.657309Z",
     "iopub.status.busy": "2024-04-09T16:39:49.656801Z",
     "iopub.status.idle": "2024-04-09T16:39:49.670115Z",
     "shell.execute_reply": "2024-04-09T16:39:49.669085Z"
    },
    "id": "hxFLzyRX2EPu",
    "papermill": {
     "duration": 0.031053,
     "end_time": "2024-04-09T16:39:49.672900",
     "exception": false,
     "start_time": "2024-04-09T16:39:49.641847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self,\n",
    "                 policy,\n",
    "                 optimizer,\n",
    "                 scheduler,\n",
    "                 value_loss_coef=0.25,\n",
    "                 entropy_coef=0.01,\n",
    "                 max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory):\n",
    "\n",
    "        advantages = trajectory['value_targets'] \\\n",
    "            - trajectory['values'].detach()\n",
    "\n",
    "        log_probs = trajectory['log_probs']\n",
    "        policy_loss = -(advantages * log_probs).mean()\n",
    "        trajectory['policy_loss'] = policy_loss\n",
    "        trajectory['advantages'] = advantages\n",
    "        return policy_loss\n",
    "\n",
    "    def value_loss(self, trajectory):\n",
    "        value_targets = trajectory['value_targets']\n",
    "        values = trajectory['values']\n",
    "        value_loss = F.mse_loss(values, value_targets)\n",
    "        trajectory['value_loss'] = value_loss\n",
    "        return value_loss\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        log_probs = trajectory['log_probs']\n",
    "        \n",
    "        trajectory['entropy'] = trajectory['entropy'].mean()\n",
    "        entropy = trajectory['entropy'] * self.entropy_coef\n",
    "       \n",
    "        return self.policy_loss(trajectory) + self.value_loss(trajectory) * self.value_loss_coef \\\n",
    "            - entropy\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        loss = self.loss(trajectory)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b118f4",
   "metadata": {
    "id": "JIMtFZuG2EPu",
    "papermill": {
     "duration": 0.01132,
     "end_time": "2024-04-09T16:39:49.695435",
     "exception": false,
     "start_time": "2024-04-09T16:39:49.684115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now you can train your model. With reasonable hyperparameters training on a single GTX1080 for 10 million steps across all batched environments (which translates to about 5 hours of wall clock time)\n",
    "it should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last\n",
    "episodes in each environment in the batch) of about 600. You should plot this quantity with respect to\n",
    "`runner.step_var` &mdash; the number of interactions with all environments. It is highly\n",
    "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
    "\n",
    "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between\n",
    "value targets and value predictions\n",
    "* Entropy of the policy $\\pi$\n",
    "* Value loss\n",
    "* Policy loss\n",
    "* Value targets\n",
    "* Value predictions\n",
    "* Gradient norm\n",
    "* Advantages\n",
    "* A2C loss\n",
    "\n",
    "For optimization we suggest you use RMSProp with learning rate starting from 7e-4 and linearly decayed to 0, smoothing constant (alpha in PyTorch and decay in TensorFlow) equal to 0.99 and epsilon equal to 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93007ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:56:17.456175Z",
     "start_time": "2024-04-07T14:56:15.603592Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:49.719843Z",
     "iopub.status.busy": "2024-04-09T16:39:49.718917Z",
     "iopub.status.idle": "2024-04-09T16:39:52.193567Z",
     "shell.execute_reply": "2024-04-09T16:39:52.192550Z"
    },
    "id": "RhMMomxAqUOi",
    "outputId": "62dbcc47-1f13-4bd3-bb2e-3f15eab3ed8f",
    "papermill": {
     "duration": 2.489105,
     "end_time": "2024-04-09T16:39:52.195712",
     "exception": false,
     "start_time": "2024-04-09T16:39:49.706607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "num_iter =  int(1.4 * 10 ** 7 // (runner.nsteps * runner.nenvs))\n",
    "log_step = 100\n",
    "save_step =  10000\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=3e-4, alpha=0.99, eps=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=num_iter)\n",
    "a2c = A2C(policy, optimizer, lr_scheduler, value_loss_coef=0.25, entropy_coef=0.02, max_grad_norm=0.6)\n",
    "\n",
    "num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68668b0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-07T14:56:17.458584Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T16:39:52.220385Z",
     "iopub.status.busy": "2024-04-09T16:39:52.219887Z",
     "iopub.status.idle": "2024-04-10T02:20:35.935439Z",
     "shell.execute_reply": "2024-04-10T02:20:35.934667Z"
    },
    "id": "shp2YKiAqUOi",
    "is_executing": true,
    "outputId": "ca5466b6-a955-45da-95af-47fb76475f4d",
    "papermill": {
     "duration": 34843.730242,
     "end_time": "2024-04-10T02:20:35.937463",
     "exception": false,
     "start_time": "2024-04-09T16:39:52.207221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/700000 [00:00<?, ?it/s]/tmp/ipykernel_26/581657084.py:33: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  value_targets = self._calc_values(last_value.float(), torch.tensor(resets,  device=device), torch.tensor(rewards, dtype=torch.float32, device=device))\n",
      "/tmp/ipykernel_26/581657084.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  value_targets = self._calc_values(last_value.float(), torch.tensor(resets,  device=device), torch.tensor(rewards, dtype=torch.float32, device=device))\n",
      "/opt/conda/lib/python3.10/site-packages/gymnasium/core.py:297: UserWarning: \u001b[33mWARN: env.get_values to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_values` for environment variables or `env.get_attr('get_values')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "100%|██████████| 700000/700000 [9:40:43<00:00, 20.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "for step in trange(num_iter):\n",
    "\n",
    "    trajectory = runner.get_next()\n",
    "    loss = a2c.step(trajectory)\n",
    "    \n",
    "    \n",
    "    if step % log_step == 0:\n",
    "        value_loss = trajectory['value_loss']\n",
    "        policy_loss = trajectory['policy_loss']\n",
    "        entropy = trajectory['entropy']\n",
    "\n",
    "        value_targets = trajectory['value_targets'].mean()\n",
    "        values = trajectory['values'].mean()\n",
    "        advantages = trajectory['advantages'].mean()\n",
    "        rewards = [[0, 0]] + env.get_values(\"Episodes/reward_mean_100\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"step\": step,\n",
    "            \"reward_mean_100\": rewards[-1][-1],\n",
    "            \"loss\": loss,\n",
    "            \"values\": values,\n",
    "            \"value_targets\": value_targets,\n",
    "            \"value_loss\": value_loss,\n",
    "            \"policy_loss\": policy_loss,\n",
    "            \"entropy\": entropy,\n",
    "            \"advantages\": advantages,\n",
    "            \n",
    "        })\n",
    "    \n",
    "    if step % save_step == 0:\n",
    "        torch.save(model.state_dict(), \"last_state_dict.pt\")\n",
    "        wandb.save(\"last_state_dict.pt\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34eb79f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T02:21:19.157621Z",
     "iopub.status.busy": "2024-04-10T02:21:19.157232Z",
     "iopub.status.idle": "2024-04-10T02:21:22.237296Z",
     "shell.execute_reply": "2024-04-10T02:21:22.236580Z"
    },
    "papermill": {
     "duration": 24.58487,
     "end_time": "2024-04-10T02:21:22.239304",
     "exception": false,
     "start_time": "2024-04-10T02:20:57.654434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      advantages ▆▆▇▆██▇▇▇▆▆▇▆▄▁▇▇▇▇▆▅█▆▇▇▆▇▇▆▇▇█▇▅▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         entropy ▂▆█▇▇▇▆▅▇▅▅▅█▅▆▆▆▃▅▅▅▅▂▅▄▅▃▁▅▇▃▅██▇▅▇▃█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss ▅▄▅▄▆▇▅▅▅▃▃▄▃▃▁▅▅▅▅▄▄▇▅▅▅▄█▆▄▅▅▆▅▃▅▆▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     policy_loss ▇▇▇▇██▇▇▇▆▆▇▆▅▁▇▇▇▇▆▆█▇▇▇▆█▇▆▇▇▇▇▅▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: reward_mean_100 ▁▁▂▂▂▂▃▄▃▃▄▄▅▅▄▆▆▄▅▇▅▆▄▄▅▅▄▅▅▆▆▆▇▆▅▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      value_loss ▁▁▁▁▁▂▁▁▁▁▁▁▁▂█▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   value_targets ▆▃▂▅▄▄▄▃▃▅▇▄▃▃▄▆▃▇▆▆▃▂▄▅▇▂▇█▆▇▃▅▁▃▄▄▁▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          values ▆▃▂▅▄▄▄▃▃▅▇▄▃▃▅▆▃▇▆▆▄▂▄▅▇▂▇█▆▇▃▅▁▃▄▄▁▃▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      advantages -0.15014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         entropy 1.62476\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            loss -0.2734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     policy_loss -0.25878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: reward_mean_100 465.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            step 699900\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      value_loss 0.07148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   value_targets 2.95976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          values 3.1099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmisty-voice-38\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/hallloy/a2c/runs/zk0z1472\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240409_163917-zk0z1472/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfca00",
   "metadata": {
    "id": "ZDbgUdMq2EPu",
    "papermill": {
     "duration": 22.174254,
     "end_time": "2024-04-10T02:22:06.364288",
     "exception": false,
     "start_time": "2024-04-10T02:21:44.190034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Target networks?\n",
    "\n",
    "You may recall a technique called \"target networks\" we used a few weeks ago when we trained a DQN agent to play Atari Breakout and wonder why we have not suggested using them here. The answer is that this is more historical than practical.\n",
    "\n",
    "While the \"chasing the target\" problem is still present in actor-critic value estimation and target networks do show up in follow-up papers, the original A3C/A2C papers do not mention them and do not explain this omission.\n",
    "\n",
    "The hypothesis why this may not be a big deal (compared to Q-learning) goes like this. An A3C/A2C agent selects actions based on policy, not an epsilon greedy exploration function, for which the argmax can change drastically due to tiny errors in function approximation. Therefore, errors in the value target caused by target chasing will cause less damage.\n",
    "\n",
    "Also, the actor-critic gradient relies on the advantage function $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$. Compare this to the $Q$-function $Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\cdot \\mathbb{E}_{s_{t+1} \\mid s_t, a_t} V(s_{t+1})$ used in Q-learning and SARSA: we would expect that any bias in $V$-function approximation will be carried over from $V(s_{t+1})$ to $V(s_t)$ by gradient updates. However, in the formula for the advantage function the two approximations ($Q$-function and $V$-function) come with opposite signs, and thus the errors will cancel out.\n",
    "\n",
    "The last reason may be computational. Authors were concerned to beat existent algorithms in the wall-clock learning time, and any overhead of parameter copying (target network update) counted against this goal."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelInstanceId": 23637,
     "sourceId": 28076,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35065.761414,
   "end_time": "2024-04-10T02:22:30.417818",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-09T16:38:04.656404",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
